## Setup
```{r message=FALSE, warning=FALSE}
# Load libraries
library(readr)
library(yaml)
library(data.table)
library(mvtnorm)
library(coda)
library(ggplot2)
library(devtools)
library(utils)
load_all()

# Aesthetics
theme_set(theme_bw())
```

## Loading and preparing the data
```{r message=FALSE, warning=FALSE}
# Load the data
df <- read_rds(file.path("../", "data", "silver", "covimod_varselect_wave_21.rds"))

# Load configurations
config <- read_yaml("../config/varselect.yaml")
```

```{r}
# Make stan data
stan_data <- make_stan_data_varselect(df, config, remove_first_dummy = FALSE)

# Unpack the data
y <- stan_data$y
X <- stan_data$X
```

## The model: Poisson model
In this analysis, I will consider two standard count models: a Poisson model and a negative binomial model. I will start with the Poisson model for which it is easier to implement the Metropolis-Hastings MCMC algorithm then move on to the more complicated negative binomial model. 

Let $y_i$ for $i = 1,2,\ldots,n$ denote the number of contacts for individual $i$ within a 24 hour period. Let $\boldsymbol{x}_i = (x_{i1}, x_{i2}, \ldots, x_{ip})^\top$ denote the vector of covariates for individual $i$. The Poisson model is given by
$$
\begin{align}
y_i &\sim \text{Poisson}(\lambda_i) \\
\log(\lambda_i) &= \beta_0 + \boldsymbol{x}_i^\top (\boldsymbol{\gamma} \odot \boldsymbol{\beta})
\end{align}
$$
where $\beta_0$ is the intercept, $\boldsymbol{\beta} = (\beta_1, \beta_2, \ldots, \beta_p)$ is the vector of coefficients, and $\boldsymbol{\gamma} = (\gamma_1, \gamma_2, \ldots, \gamma_p)$ is the vector of selection indicators. The selection indicators are binary variables that indicate whether the corresponding covariate is included in the model. The symbol $\odot$ denotes the element-wise product.

The prior distribution for the coefficients is given by
$$
\begin{align}
\beta_0 &\sim \text{Normal}(0, 16) \\
\beta_j &\sim \text{Normal}(0, 4) \quad \text{for} \quad j = 1,2,\ldots,p \\
\gamma_j &\sim \text{Bernoulli}(1/2) \quad \text{for} \quad j = 1,2,\ldots,p
\end{align}
$$

## The Metropolis MCMC algorithm
The Metropolis-Hastings MCMC algorithm is given by the following steps:

1. Initialize the coefficients $\beta_0, \beta_1, \ldots, \beta_p$ and the selection indicators $\gamma_1, \gamma_2, \ldots, \gamma_p$.
2. For $s = 1,2,\ldots,S$ do:
    1. Sample $\boldsymbol{\gamma}$
        a. Set $\boldsymbol{\gamma} = \boldsymbol{\gamma}^{(s)}$
        b. In a random order for $j \in \{1,2,\ldots,p\}$ set $\gamma^*_j = 1 - \gamma^{(s)}_j$.
        c. Evaluate $$
        r_{\gamma_j} = \frac{
          p(\boldsymbol{y} | \boldsymbol{X}, \beta_0^{(s)}, \boldsymbol{\beta}^{(s)}, \boldsymbol{\gamma}^{(s)}_{-j}, \gamma_j = \gamma^*_j)p(\gamma^*_j)}{
          p(\boldsymbol{y} | \boldsymbol{X}, \beta_0^{(s)}, \boldsymbol{\beta}^{(s)}, \boldsymbol{\gamma}^{(s)}_{-j}, \gamma_j = \gamma^{(s)}_j)p(\gamma^{(s)}_j)
        }$$
        d. Set $\gamma^{(s)}_j = \gamma^*_j$ with probability $\min(1, r_{\gamma_j})$ and set $\gamma^{(s)}_j = \gamma^{(s-1)}_j$ with probability $1 - \min(1, r_{\gamma_j}$.
    2. Sample $\beta_0$
        a. Propose $\beta_0^* \sim \text{Uniform}(\beta_0^{(s)} - \delta_{\beta_0}, \beta_0^{(s)} + \delta_{\beta_0})$.
        b. Evaluate $$
        r_{\beta_0} = \frac{
          p(\boldsymbol{y} | \boldsymbol{X}, \beta_0^*, \boldsymbol{\beta}^{(s-1)}, \boldsymbol{\gamma}^{(s)})p(\beta_0^*)}{
          p(\boldsymbol{y} | \boldsymbol{X}, \beta_0^{(s-1)}, \boldsymbol{\beta}^{(s-1)}, \boldsymbol{\gamma}^{(s)})p(\beta_0^{(s-1)})
        }$$
        d. Set $\beta_0^{(s)} = \beta_0^*$ with probability $\min(1, r_{\beta_0})$ and set $\beta_0^{(s)} = \beta_0^{(s-1)}$ with probability $1 - \min(1, r_{\beta_0})$.
    3. Sample $\boldsymbol{\beta}$
        a. Propose $\boldsymbol{\beta}^* \sim \text{Normal}(\boldsymbol{\beta}^{(s-1)}, \delta_{\beta} \times \mathbb{I_p})$.
        b. Evaluate $$
        r_{\boldsymbol{\beta}} = \frac{
          p(\boldsymbol{y} | \boldsymbol{X}, \beta_0^{(s)}, \boldsymbol{\beta}^*, \boldsymbol{\gamma}^{(s)})p(\boldsymbol{\beta}^*)}{
          p(\boldsymbol{y} | \boldsymbol{X}, \beta_0^{(s)}, \boldsymbol{\beta}^{(s-1)}, \boldsymbol{\gamma}^{(s)})p(\boldsymbol{\beta}^{(s-1)})
        }$$
        c. Set $\boldsymbol{\beta}^{(s)} = \boldsymbol{\beta}^*$ with probability $\min(1, r_{\boldsymbol{\beta}})$ and set $\boldsymbol{\beta}^{(s)} = \boldsymbol{\beta}^{(s-1)}$ with probability $1 - \min(1, r_{\boldsymbol{\beta}})$.

Define some helper functions
```{r}
# Log-likelihood function
poisson_glm_log_lpmf <- function(y, X, beta0, beta, gamma) {
  lambda <- exp(beta0 + X %*% (gamma * beta))
  return(sum(dpois(y, lambda, log = TRUE)))
}
```

Instantiate the objects to store the MCMC samples
```{r}
S <- 10000
BETA0 <- array(NA, dim = S)
BETA <- array(NA, dim = c(S, ncol(X)))
GAMMA <- array(NA, dim = c(S, ncol(X)))
```

Set the hyperparameters for the proposal distributions
```{r}
d_b0 <- 0.1
d_b <- 0.1
```

Run the Metropolis MCMC algorithm
```{r}
# Set seed
set.seed(123)

# Set initial values
g <- rep(1, ncol(X))
b0 <- log(mean(y))
beta <- rnorm(ncol(X), 0, 0.1)

# Progress bar
pb <- txtProgressBar(min = 0, max = S, style = 3)
for (s in 1:S) {
  # ===== Sample gamma =====
  for (j in sample(1:ncol(X))) {
    gp <- g
    gp[j] <- 1 - gp[j]
    lrg <- poisson_glm_log_lpmf(y, X, b0, beta, gp) - poisson_glm_log_lpmf(y, X, b0, beta, g)
    if (log(runif(1)) < lrg) { g[j] <- gp[j] }
    GAMMA[s,j] <- g[j]
  }
  
  # ===== Sample beta0 =====
  b0p <- runif(1, b0 - d_b0, b0 + d_b0)
  lrb0 <- poisson_glm_log_lpmf(y, X, b0p, beta, g) - poisson_glm_log_lpmf(y, X, b0, beta, g) + 
    dnorm(b0p, 0, 4, log = TRUE) - dnorm(b0, 0, 4, log = TRUE)
  if (log(runif(1)) < lrb0) { b0 <- b0p }
  BETA0[s] <- b0
  
  # ===== Sample beta =====
  for (j in 1:ncol(X)) {
    betap <- beta
    betap[j] <- rnorm(1, beta[j], d_b)
    lrb <- poisson_glm_log_lpmf(y, X, b0, betap, g) - poisson_glm_log_lpmf(y, X, b0, beta, g) +
      dnorm(betap[j], 0, 2, log = TRUE) - dnorm(beta[j], 0, 2, log = TRUE)
    if (log(runif(1)) < lrb) { beta[j] <- betap[j] }
    BETA[s,j] <- beta[j]
  }
  
  setTxtProgressBar(pb, s)
}
close(pb)
```

```{r}
S_burnin <- 2000

# Plot the MCMC samples
BETA0 <- mcmc(BETA0[-(1:S_burnin)])
plot(BETA0)
```

```{r}
# Posterior inclusion probability
GAMMA <- GAMMA[-(1:S_burnin),]
pip <- apply(GAMMA, 2, mean)
dt_pois_pip <- data.table(varname = colnames(X), pip = pip)

# Plot the posterior inclusion probability
ggplot(dt_pois_pip, aes(x = reorder(varname, -pip), y = pip)) +
  geom_point() +
  geom_hline(yintercept = 0.5, linetype = "dashed") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Variable", y = "Posterior inclusion probability")
```

## The Model: Negative Binomial Regression
Define a helper function for the log-likelihood
```{r}
# Log-likelihood function
negbin_glm_log_lpmf <- function(y, X, beta0, beta, gamma, phi) {
  lambda <- exp(beta0 + X %*% (gamma * beta))
  return(sum(dnbinom(y, mu = lambda, size = phi, log = TRUE)))
}
```

Instantiate the objects to store the MCMC samples
```{r}
S <- 10000
BETA0 <- array(NA, dim = S)
BETA <- array(NA, dim = c(S, ncol(X)))
GAMMA <- array(NA, dim = c(S, ncol(X)))
PHI <- array(NA, dim = S)
```

Set the hyperparameters for the proposal distributions
```{r}
d_b0 <- 0.1
d_b <- 0.2
d_p <- 0.1
```

```{r}
# Set seed
set.seed(123)

# Set initial values
g <- rep(1, ncol(X))
b0 <- log(mean(y))
inv_phi <- 1/rexp(1)
beta <- rnorm(ncol(X), 0, 0.1)

# Progress bar
pb <- txtProgressBar(min = 0, max = S, style = 3)
for (s in 1:S) {
  # ===== Sample gamma =====
  for (j in sample(1:ncol(X))) {
    gp <- g
    gp[j] <- 1 - gp[j]
    lrg <- negbin_glm_log_lpmf(y, X, b0, beta, gp, 1/inv_phi) - negbin_glm_log_lpmf(y, X, b0, beta, g, 1/inv_phi)
    if (log(runif(1)) < lrg) { g[j] <- gp[j] }
    GAMMA[s,j] <- g[j]
  }
  
  # ===== Sample phi =====
  inv_phip <- runif(1, inv_phi - d_p, inv_phi + d_p)
  if (inv_phip < 0 ) { inv_phip <- -inv_phip } # Ensure phi is positive
  lrp <- negbin_glm_log_lpmf(y, X, b0, beta, g, 1/inv_phip) - negbin_glm_log_lpmf(y, X, b0, beta, g, 1/inv_phi) +
    dexp(inv_phip, 1, log = TRUE) - dexp(inv_phi, 1, log = TRUE)
  if (log(runif(1)) < lrp) { inv_phi <- inv_phip }
  PHI[s] <- 1/inv_phi
  
  # ===== Sample beta0 =====
  b0p <- runif(1, b0 - d_b0, b0 + d_b0)
  lrb0 <- negbin_glm_log_lpmf(y, X, b0p, beta, g, 1/inv_phi) - negbin_glm_log_lpmf(y, X, b0, beta, g, 1/inv_phi) + 
    dnorm(b0p, 0, 4, log = TRUE) - dnorm(b0, 0, 4, log = TRUE)
  if (log(runif(1)) < lrb0) { b0 <- b0p }
  BETA0[s] <- b0
  
  # ===== Sample beta =====
  for (j in 1:ncol(X)) {
    betap <- beta
    betap[j] <- rnorm(1, beta[j], d_b)
    lrb <- negbin_glm_log_lpmf(y, X, b0, betap, g, 1/inv_phi) - negbin_glm_log_lpmf(y, X, b0, beta, g, 1/inv_phi) +
      dnorm(betap[j], 0, 2, log = TRUE) - dnorm(beta[j], 0, 2, log = TRUE)
    if (log(runif(1)) < lrb) { beta[j] <- betap[j] }
    BETA[s,j] <- beta[j]
  }
  
  setTxtProgressBar(pb, s)
}
close(pb)
```

```{r}
plot(BETA0[-(1:S_burnin)], type = "l", xlab = "Iteration", ylab = expression(beta[0]))
```

```{r}
# Posterior inclusion probability
GAMMA <- GAMMA[-(1:S_burnin),]
pip <- apply(GAMMA, 2, mean)
dt_pois_pip <- data.table(varname = colnames(X), pip = pip)

# Plot the posterior inclusion probability
ggplot(dt_pois_pip, aes(x = reorder(varname, -pip), y = pip)) +
  geom_point() +
  geom_hline(yintercept = 0.5, linetype = "dashed") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Variable", y = "Posterior inclusion probability")
```

